{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-tMJhxRKNMtG"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LAz624eRNTXS"},"outputs":[],"source":["# install the required libraries for BLIP\n","!pip install transformers timm torch torchvision matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T4KlmpyMNYdz"},"outputs":[],"source":["import os\n","import pickle\n","import numpy as np\n","from tqdm.notebook import tqdm\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from transformers import BlipProcessor, BlipForConditionalGeneration\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.utils import to_categorical, plot_model\n","from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, Add\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.models import load_model\n","import tensorflow as tf\n","from nltk.translate.bleu_score import corpus_bleu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hUcNN0RwNfvs"},"outputs":[],"source":["\n","BASE_DIR = '/content/drive/MyDrive/Hindi Image Captioning'\n","WORKING_DIR = '/content/drive/MyDrive/Image Captioning/working'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3xD7Uu2NfsF"},"outputs":[],"source":["# Load BLIP processor and model\n","processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","model_blip = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BCAJLrulLT-W"},"outputs":[],"source":["# extract features from image\n","features = {}\n","directory = os.path.join(BASE_DIR, 'Flicker 8K Dataset/Images')\n","\n","for img_name in tqdm(os.listdir(directory)):\n","    # Load the image\n","    img_path = os.path.join(directory, img_name)\n","    image = Image.open(img_path).convert(\"RGB\")\n","\n","    # Preprocess and extract features with BLIP\n","    inputs = processor(image, return_tensors=\"pt\")\n","    feature = model_blip.vision_model(inputs[\"pixel_values\"]).last_hidden_state.mean(dim=1).detach().numpy()\n","\n","    # Store the feature\n","    image_id = img_name.split('.')[0]\n","    features[image_id] = feature"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_iXjUxqNflm"},"outputs":[],"source":["# Store features in pickle\n","pickle.dump(features, open(os.path.join(WORKING_DIR, 'features_blip.pkl'), 'wb'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8n0VOgHNfkf"},"outputs":[],"source":["# Load features from pickle\n","with open(os.path.join(WORKING_DIR, 'features_blip.pkl'), 'rb') as f:\n","    features = pickle.load(f)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuRGDVlvNfip"},"outputs":[],"source":["# Processing captions\n","with open(os.path.join(BASE_DIR, 'Hindi captions/UnClean-5Sentences_withComma.txt'), 'r') as f:\n","    next(f)\n","    captions_doc = f.read()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"meJcROlnN7Ub"},"outputs":[],"source":["# Create mapping of image to captions\n","mapping = {}\n","for line in tqdm(captions_doc.split('\\n')):\n","    # Split the line by comma(,)\n","    tokens = line.split(',')\n","    if len(line) < 2:\n","        continue\n","    image_id, caption = tokens[0], tokens[1:]\n","    image_id = image_id.split('.')[0]\n","    caption = \" \".join(caption)\n","    if image_id not in mapping:\n","        mapping[image_id] = []\n","    mapping[image_id].append(caption)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k1gm2ezCN7S5"},"outputs":[],"source":[" # create mapping of image to captions\n","mapping = {}\n","# process lines\n","for line in tqdm(captions_doc.split('\\n')):\n","    # split the line by comma(,)\n","    tokens = line.split(',')\n","    if len(line) < 2:\n","        continue\n","    image_id, caption = tokens[0], tokens[1:]\n","    # remove extension from image ID\n","    image_id = image_id.split('.')[0]\n","    # convert caption list to string\n","    caption = \" \".join(caption)\n","    # create list if needed\n","    if image_id not in mapping:\n","        mapping[image_id] = []\n","    # store the caption\n","    mapping[image_id].append(caption)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xsfXIaFsvn2s"},"outputs":[],"source":["def clean(mapping):\n","  for key, captions in mapping.items():\n","        for i in range(len(captions)):\n","            # take one caption at a time\n","            caption = captions[i]\n","            # preprocessing steps\n","            # convert to lowercase\n","            caption = caption.lower()\n","            # delete digits, special chars, etc.,\n","            caption = caption.replace('[^A-Za-z]', '')\n","            # delete additional spaces\n","            caption = caption.replace('\\s+', ' ')\n","            # add start and end tags to the caption\n","            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n","            captions[i] = caption\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOr5y8llvs0n"},"outputs":[],"source":["# before preprocess of text\n","mapping['1000268201_693b08cb0e']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GuBk8d1AvsxN"},"outputs":[],"source":["# preprocess the text\n","clean(mapping)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PdlkLop-vzf_"},"outputs":[],"source":["# after preprocess of text\n","mapping['1000268201_693b08cb0e']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7DlyfoWVvzcc"},"outputs":[],"source":["all_captions = []\n","for key in mapping:\n","    for caption in mapping[key]:\n","        all_captions.append(caption)\n","len(all_captions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZN4MxT5oN7RW"},"outputs":[],"source":["# Tokenize the text\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([caption for captions in mapping.values() for caption in captions])\n","vocab_size = len(tokenizer.word_index) + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G2NN6WMrN7Pz"},"outputs":[],"source":["# get maximum length of the caption available\n","max_length = max(len(caption.split()) for caption in all_captions)\n","max_length"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YYHnqDnIR13O"},"outputs":[],"source":["# Train-test split\n","image_ids = list(mapping.keys())\n","split = int(len(image_ids) * 0.90)\n","train = image_ids[:split]\n","test = image_ids[split:]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S6iVax8rR1zt"},"outputs":[],"source":["# Create data generator\n","def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n","    X1, X2, y = [], [], []\n","    n = 0\n","    while True:\n","        for key in data_keys:\n","            captions = mapping[key]\n","            for caption in captions:\n","                seq = tokenizer.texts_to_sequences([caption])[0]\n","                for i in range(1, len(seq)):\n","                    in_seq = pad_sequences([seq[:i]], maxlen=max_length)[0]\n","                    out_seq = to_categorical([seq[i]], num_classes=vocab_size)[0]\n","                    X1.append(features[key][0])\n","                    X2.append(in_seq)\n","                    y.append(out_seq)\n","            n += 1\n","            if n == batch_size:\n","                yield [np.array(X1), np.array(X2)], np.array(y)\n","                X1, X2, y = [], [], []\n","                n = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cpr-r1QD1z5o"},"outputs":[],"source":["from keras.utils import plot_model\n","\n","# Encoder model\n","inputs1 = Input(shape=(features[list(features.keys())[0]].shape[-1],))\n","fe1 = Dropout(0.4)(inputs1)\n","fe2 = Dense(256, activation='relu')(fe1)\n","\n","inputs2 = Input(shape=(max_length,))\n","se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n","se2 = Dropout(0.4)(se1)\n","se3 = LSTM(256)(se2)\n","\n","# Decoder model\n","decoder1 = Add()([fe2, se3])\n","decoder2 = Dense(256, activation='relu')(decoder1)\n","outputs = Dense(vocab_size, activation='softmax')(decoder2)\n","\n","model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Generate and display the diagram\n","plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RFBUItjPkh1l"},"outputs":[],"source":["# Set up the checkpoint directory\n","checkpoint_dir = './training_checkpoint_1'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","\n","# Create the checkpoint callback\n","checkpoint_callback = ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True,\n","                                      monitor='val_accuracy',\n","                                      mode='max',save_best_only=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXITqKA_baTs"},"outputs":[],"source":["import os\n","import tensorflow as tf\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","# Set up the checkpoint directory\n","checkpoint_dir = './training_checkpoint_1'\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","\n","# Create the checkpoint callback\n","checkpoint_callback = ModelCheckpoint(filepath=checkpoint_prefix,\n","                                      save_weights_only=True,\n","                                      monitor='val_accuracy',\n","                                      mode='max',\n","                                      save_best_only=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0QXnuo_rkuPQ"},"outputs":[],"source":["\n","# Define the number of epochs and batch size\n","epochs =10\n","batch_size = 32\n","steps = len(train) // batch_size\n","\n","# Check if there are existing checkpoints and load the model\n","initial_epoch = 0\n","if os.path.exists(checkpoint_dir):\n","    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n","    if latest_checkpoint:\n","        print(f\"Resuming from {latest_checkpoint}\")\n","        model.load_weights(latest_checkpoint)\n","        initial_epoch = int(latest_checkpoint.split('-')[-1]) + 1\n","\n","# Train the model and store the history\n","history = model.fit(\n","    data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size),\n","    epochs=epochs,\n","    steps_per_epoch=steps,\n","    initial_epoch=initial_epoch,\n","    callbacks=[checkpoint_callback],\n","    verbose=1\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TK7fed_SSEdu"},"outputs":[],"source":["# Save the model\n","model.save(os.path.join(WORKING_DIR, 'best_model_blip_o1.h5'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uj8fOh2Prw7Q"},"outputs":[],"source":["# Function to plot the accuracy and loss graphs\n","def plot_history(history):\n","    # Plot accuracy\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(history.history['accuracy'], label='accuracy')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.title('Model Accuracy')\n","    plt.legend()\n","\n","    # Plot loss\n","    plt.subplot(1, 2, 2)\n","    plt.plot(history.history['loss'], label='loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.title('Model Loss')\n","    plt.legend()\n","\n","    plt.show()\n","\n","# Plot the history\n","plot_history(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E0f2ZwmnSEaW"},"outputs":[],"source":["# Function to predict captions\n","def predict_caption(model, image, tokenizer, max_length):\n","    in_text = 'startseq'\n","    for i in range(max_length):\n","        sequence = tokenizer.texts_to_sequences([in_text])[0]\n","        sequence = pad_sequences([sequence], max_length)\n","        yhat = model.predict([image, sequence], verbose=0)\n","        yhat = np.argmax(yhat)\n","        word = next((w for w, idx in tokenizer.word_index.items() if idx == yhat), None)\n","        if word is None or word == 'endseq':\n","            break\n","        in_text += \" \" + word\n","    return in_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1VQPhqVgffoh"},"outputs":[],"source":["# Calculate BLEU scores for model evaluation\n","actual, predicted = [], []\n","for key in tqdm(test):\n","    captions = mapping[key]\n","    y_pred = predict_caption(model, features[key], tokenizer, max_length).split()\n","    actual_captions = [caption.split() for caption in captions]\n","    actual.append(actual_captions)\n","    predicted.append(y_pred)\n","\n","print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n","print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n","print(\"BLEU-3: %f\" % corpus_bleu(actual, predicted, weights=(1/3, 1/3, 1/3, 0)))\n","print(\"BLEU-4: %f\" % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zC1zOzwpE9A"},"outputs":[],"source":["import os\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from tkinter import Tk, filedialog\n","\n","# Function to open a file dialog and select an image\n","def select_image():\n","    root = Tk()\n","    root.withdraw()  # Hide the root window\n","    file_path = filedialog.askopenfilename(\n","        initialdir=os.path.expanduser(\"~/Desktop\"),\n","        title=\"Select an image\",\n","        filetypes=((\"jpeg files\", \"*.jpg\"), (\"all files\", \"*.*\"))\n","    )\n","    return file_path\n","\n","# Function to generate caption for the selected image\n","def generate_caption(image_path):\n","    image_name = os.path.basename(image_path)\n","    image_id = image_name.split('.')[0]\n","    img_path = os.path.join(BASE_DIR, \"Flicker 8K Dataset/Images\", image_name)\n","    image = Image.open(img_path)\n","\n","    print('--- Actual Captions ---')\n","    for caption in mapping[image_id]:\n","        print(caption)\n","\n","    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n","    print('--- Predicted Caption ---')\n","    print(y_pred)\n","\n","    plt.imshow(image)\n","    plt.show()\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    image_path = select_image()\n","    if image_path:\n","        generate_caption(image_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_QKj_iACjzed"},"outputs":[],"source":["# Function to generate caption for a given image\n","def generate_caption(image_name):\n","    image_id = image_name.split('.')[0]\n","    img_path = os.path.join(BASE_DIR, \"Flicker 8K Dataset/Images\", image_name)\n","    image = Image.open(img_path)\n","    print('--- Actual Captions ---')\n","    for caption in mapping[image_id]:\n","        print(caption)\n","\n","    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n","    print('--- Predicted Caption ---')\n","    print(y_pred)\n","\n","    plt.imshow(image)\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"St8JaJ_Z95S6"},"outputs":[],"source":["generate_caption(\"1057251835_6ded4ada9c.jpg\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LeEewtqm-D8o"},"outputs":[],"source":["generate_caption(\"1191338263_a4fa073154.jpg\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAlJvT-P-fGt"},"outputs":[],"source":["generate_caption(\"1343426964_cde3fb54e8.jpg\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_VHviwOd-fDR"},"outputs":[],"source":["generate_caption(\"1404832008_68e432665b.jpg\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-UyGPJ76Bh3T"},"outputs":[],"source":["generate_caption(\"109202801_c6381eef15.jpg\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}